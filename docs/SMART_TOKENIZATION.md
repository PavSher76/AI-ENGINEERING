# Умная система токенизации документов

## Обзор

Реализована интеллектуальная система токенизации документов с применением RAG-сервиса для анализа и обработки загружаемых файлов. Система обеспечивает умное разделение документа на токены с сохранением контекста и направляет токенизированный текст на анализ LLM согласно запросам пользователя.

## Основные компоненты

### 1. SmartTokenizer (`services/chat-service/services/smart_tokenizer.py`)

**Функциональность:**
- Умное разделение документа на семантические блоки
- Сохранение контекста при разбиении
- Определение типа контента (определения, требования, списки, таблицы)
- Расчет важности фрагментов
- Извлечение ключевых слов
- Поддержка перекрытий между чанками

**Ключевые возможности:**
- Анализ структуры документа (заголовки, разделы, подразделы)
- Классификация типов контента
- Определение языка документа
- Оптимизация размеров чанков
- Обогащение метаданными

### 2. RAGIntegrationService (`services/chat-service/services/rag_integration_service.py`)

**Функциональность:**
- Интеграция с RAG-сервисом для анализа документов
- Поиск релевантных фрагментов по запросу
- Анализ токенизированных чанков с помощью LLM
- Комплексный анализ документов

**Ключевые возможности:**
- Загрузка чанков в RAG систему
- Гибридный поиск (векторный + BM25)
- Re-ranking результатов
- Генерация структурированных ответов

## API Эндпоинты

### 1. Токенизация документа
```http
POST /analyze/tokenize
Content-Type: multipart/form-data

files: [файл для токенизации]
```

**Ответ:**
```json
{
  "success": true,
  "filename": "document.pdf",
  "document_structure": {
    "title": "Название документа",
    "sections": [...],
    "total_tokens": 1500,
    "chunk_count": 5,
    "document_type": "technical_specification",
    "language": "ru"
  },
  "token_chunks": [
    {
      "chunk_id": "chunk_0_20251001_131558",
      "text": "Содержимое чанка",
      "token_count": 300,
      "chunk_type": "requirement",
      "importance_score": 0.8,
      "context_keywords": ["ключевые", "слова"]
    }
  ],
  "statistics": {
    "total_chunks": 5,
    "total_tokens": 1500,
    "average_chunk_size": 300.0,
    "chunk_types_distribution": {
      "requirement": 3,
      "definition": 2
    }
  }
}
```

### 2. Анализ документа с RAG
```http
POST /analyze/document
Content-Type: multipart/form-data

message: "Запрос пользователя"
session_id: "session_123"
analysis_type: "requirements"
files: [файл для анализа]
```

**Типы анализа:**
- `summary` - Краткое изложение
- `key_points` - Ключевые моменты
- `requirements` - Требования
- `definitions` - Определения
- `recommendations` - Рекомендации
- `compliance` - Соответствие стандартам
- `risks` - Риски и проблемы

**Ответ:**
```json
{
  "success": true,
  "analysis_result": {
    "query": "Запрос пользователя",
    "rag_analysis": {
      "confidence": 0.85,
      "sources_count": 3,
      "sources": [...]
    },
    "llm_analysis": {
      "analysis_type": "requirements",
      "summary": "Краткое изложение",
      "key_points": ["Пункт 1", "Пункт 2"],
      "recommendations": ["Рекомендация 1"],
      "confidence": 0.9
    },
    "document_stats": {
      "total_chunks": 5,
      "total_tokens": 1500,
      "average_importance": 0.75
    }
  }
}
```

### 3. Получение результатов анализа
```http
GET /analyze/sessions/{session_id}
```

## Алгоритм работы

### 1. Токенизация документа

1. **Предварительная обработка:**
   - Нормализация пробелов и символов
   - Удаление служебных символов
   - Нормализация переносов строк

2. **Анализ структуры:**
   - Определение заголовков и разделов
   - Классификация типов контента
   - Определение языка документа

3. **Семантическое разбиение:**
   - Извлечение абзацев
   - Определение типа контента
   - Расчет важности фрагментов
   - Извлечение ключевых слов

4. **Токенизация блоков:**
   - Разбиение больших блоков на чанки
   - Создание перекрытий между чанками
   - Обогащение метаданными

5. **Оптимизация:**
   - Объединение маленьких чанков
   - Оптимизация размеров

### 2. RAG анализ

1. **Загрузка в RAG:**
   - Создание коллекции документов
   - Загрузка чанков как отдельных документов
   - Создание эмбеддингов

2. **Поиск релевантных фрагментов:**
   - Улучшение запроса для типа анализа
   - Векторный поиск
   - BM25 поиск
   - Объединение результатов

3. **LLM анализ:**
   - Подготовка контекста
   - Формирование промпта
   - Отправка запроса к LLM
   - Парсинг ответа

## Настройки

### SmartTokenizer
```python
max_chunk_size = 1000      # Максимальное количество токенов в чанке
overlap_size = 200         # Размер перекрытия между чанками
min_chunk_size = 100       # Минимальный размер чанка
```

### RAGIntegrationService
```python
max_chunks_for_analysis = 10    # Максимальное количество чанков для анализа
min_confidence_threshold = 0.3  # Минимальный порог уверенности
analysis_timeout = 30.0         # Таймаут анализа в секундах
```

## Поддерживаемые форматы

- **PDF** - с поддержкой OCR для растровых документов
- **DOCX** - с извлечением текста и таблиц
- **XLS/XLSX** - с обработкой всех листов
- **TXT** - простой текст
- **Markdown** - с сохранением структуры

## Типы контента

Система автоматически определяет следующие типы контента:

- `definition` - Определения и термины
- `requirement` - Требования и обязательства
- `table` - Таблицы
- `list` - Списки
- `formula` - Математические формулы
- `reference` - Ссылки на документы
- `paragraph` - Обычные абзацы

## Важность фрагментов

Система рассчитывает важность фрагментов на основе:

- Ключевых слов важности (должен, обязательно, запрещается)
- Длины текста
- Наличия цифр и специальных символов
- Типа контента

## Интеграция с существующими сервисами

### RAG Service
- Использует эндпоинты `/collections/`, `/documents/upload`, `/documents/search`
- Требует настройки аутентификации для полной функциональности

### Ollama Service
- Использует эндпоинт `/api/generate` для LLM анализа
- Поддерживает модель `llama3.1:8b`

## Примеры использования

### 1. Токенизация технического задания
```bash
curl -X POST "http://localhost:8003/analyze/tokenize" \
  -F "files=@technical_specification.pdf"
```

### 2. Анализ требований
```bash
curl -X POST "http://localhost:8003/analyze/document" \
  -F "message=Какие требования к безопасности?" \
  -F "files=@security_requirements.pdf" \
  -F "analysis_type=requirements"
```

### 3. Извлечение определений
```bash
curl -X POST "http://localhost:8003/analyze/document" \
  -F "message=Найди все определения терминов" \
  -F "files=@glossary.pdf" \
  -F "analysis_type=definitions"
```

## Производительность

- **Токенизация:** ~1-2 секунды для документа 10-20 страниц
- **RAG анализ:** ~3-5 секунд в зависимости от сложности запроса
- **LLM анализ:** ~2-4 секунды для анализа 5-10 чанков

## Ограничения

- Максимальный размер файла: 100 МБ
- Максимальное количество токенов в чанке: 1000
- Таймаут анализа: 30 секунд
- Требует настройки аутентификации для RAG-сервиса

## Будущие улучшения

1. **Поддержка дополнительных форматов:**
   - PowerPoint презентации
   - HTML документы
   - XML файлы

2. **Улучшение алгоритмов:**
   - Более точная классификация контента
   - Улучшенное определение важности
   - Поддержка многоязычных документов

3. **Интеграция:**
   - Поддержка дополнительных LLM моделей
   - Интеграция с внешними базами знаний
   - Поддержка реального времени

4. **Аналитика:**
   - Метрики качества токенизации
   - Статистика использования
   - A/B тестирование алгоритмов

