"""
–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å RAG –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

import asyncio
import logging
import aiohttp
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import json

from .smart_tokenizer import TokenChunk, DocumentStructure, SmartTokenizer

logger = logging.getLogger(__name__)

@dataclass
class RAGAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ RAG"""
    query: str
    relevant_chunks: List[TokenChunk]
    rag_response: Dict[str, Any]
    confidence_score: float
    analysis_time: float
    sources: List[Dict[str, Any]]

@dataclass
class LLMAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ LLM"""
    analysis_type: str
    summary: str
    key_points: List[str]
    recommendations: List[str]
    confidence: float
    processing_time: float

class RAGIntegrationService:
    """–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å RAG –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.rag_service_url = "http://rag-service:8001"
        self.ollama_service_url = "http://ollama-service:8012"
        self.tokenizer = SmartTokenizer()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.max_chunks_for_analysis = 10
        self.min_confidence_threshold = 0.3
        self.analysis_timeout = 30.0
        
        # –¢–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞
        self.analysis_types = {
            'summary': '–ö—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
            'key_points': '–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏ –≤—ã–≤–æ–¥—ã',
            'requirements': '–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞',
            'definitions': '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ç–µ—Ä–º–∏–Ω—ã',
            'recommendations': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è',
            'compliance': '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º',
            'risks': '–†–∏—Å–∫–∏ –∏ –ø—Ä–æ–±–ª–µ–º—ã'
        }
    
    async def analyze_document_with_rag(
        self, 
        document_text: str, 
        user_query: str,
        filename: str = None,
        analysis_type: str = 'general'
    ) -> RAGAnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG
        
        Args:
            document_text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            analysis_type: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ RAG
        """
        start_time = datetime.now()
        logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º RAG –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {filename}")
        
        try:
            # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            token_chunks, document_structure = await self.tokenizer.tokenize_document(document_text, filename)
            logger.info(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(token_chunks)} —á–∞–Ω–∫–æ–≤")
            
            # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –≤ RAG —Å–∏—Å—Ç–µ–º—É
            collection_id = await self._upload_chunks_to_rag(token_chunks, document_structure)
            logger.info(f"üì§ –ß–∞–Ω–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ RAG –∫–æ–ª–ª–µ–∫—Ü–∏—é: {collection_id}")
            
            # 3. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            relevant_chunks = await self._search_relevant_chunks(user_query, collection_id, analysis_type)
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(relevant_chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # 4. –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç RAG
            rag_response = await self._get_rag_response(user_query, collection_id)
            
            # 5. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            analysis_time = (datetime.now() - start_time).total_seconds()
            
            result = RAGAnalysisResult(
                query=user_query,
                relevant_chunks=relevant_chunks,
                rag_response=rag_response,
                confidence_score=rag_response.get('confidence', 0.0),
                analysis_time=analysis_time,
                sources=rag_response.get('sources', [])
            )
            
            logger.info(f"‚úÖ RAG –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {analysis_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ RAG –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            raise
    
    async def _upload_chunks_to_rag(self, chunks: List[TokenChunk], structure: DocumentStructure) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        try:
            collection_id = f"document_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            async with aiohttp.ClientSession() as session:
                collection_data = {
                    "name": collection_id,
                    "description": f"–ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {structure.title}",
                    "type": "document_analysis"
                }
                
                async with session.post(
                    f"{self.rag_service_url}/collections/",
                    json=collection_data
                ) as response:
                    if response.status != 200:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é: {response.status}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏ –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            for i, chunk in enumerate(chunks):
                try:
                    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —á–∞–Ω–∫–∞
                    chunk_filename = f"chunk_{i}_{chunk.chunk_id}.txt"
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                    form_data = aiohttp.FormData()
                    form_data.add_field('file', chunk.text.encode('utf-8'), filename=chunk_filename, content_type='text/plain')
                    form_data.add_field('collection_id', collection_id)
                    form_data.add_field('project_id', 'document_analysis')
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"{self.rag_service_url}/documents/upload",
                            data=form_data
                        ) as response:
                            if response.status == 200:
                                logger.debug(f"–ß–∞–Ω–∫ {i} –∑–∞–≥—Ä—É–∂–µ–Ω –≤ RAG")
                            else:
                                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–∞ {i}: {response.status}")
                
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–∞ {i}: {str(e)}")
                    continue
            
            return collection_id
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–æ–≤ –≤ RAG: {str(e)}")
            raise
    
    async def _search_relevant_chunks(
        self, 
        query: str, 
        collection_id: str, 
        analysis_type: str
    ) -> List[TokenChunk]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
            enhanced_query = self._enhance_query_for_analysis(query, analysis_type)
            
            search_request = {
                "query": enhanced_query,
                "collection_id": collection_id,
                "limit": self.max_chunks_for_analysis,
                "threshold": self.min_confidence_threshold
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.rag_service_url}/documents/search",
                    json=search_request
                ) as response:
                    if response.status == 200:
                        search_result = await response.json()
                        
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ TokenChunk
                        relevant_chunks = []
                        for i, doc in enumerate(search_result.get('results', [])):
                            chunk = TokenChunk(
                                chunk_id=f"rag_chunk_{i}",
                                text=doc.get('content', ''),
                                token_count=len(doc.get('content', '').split()),
                                chunk_type='rag_result',
                                metadata={
                                    'doc_id': doc.get('id'),
                                    'title': doc.get('title'),
                                    'score': search_result.get('scores', [0])[i] if i < len(search_result.get('scores', [])) else 0,
                                    'source': 'rag_search'
                                },
                                start_position=0,
                                end_position=len(doc.get('content', '')),
                                importance_score=search_result.get('scores', [0])[i] if i < len(search_result.get('scores', [])) else 0
                            )
                            relevant_chunks.append(chunk)
                        
                        return relevant_chunks
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ RAG: {response.status}")
                        return []
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {str(e)}")
            return []
    
    def _enhance_query_for_analysis(self, query: str, analysis_type: str) -> str:
        """–£–ª—É—á—à–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        enhancements = {
            'summary': f"{query} –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã",
            'key_points': f"{query} –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤–∞–∂–Ω—ã–µ –≤—ã–≤–æ–¥—ã",
            'requirements': f"{query} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–æ—Ä–º—ã",
            'definitions': f"{query} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω—ã –ø–æ–Ω—è—Ç–∏—è",
            'recommendations': f"{query} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–æ–≤–µ—Ç—ã",
            'compliance': f"{query} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –Ω–æ—Ä–º–∞—Ç–∏–≤—ã",
            'risks': f"{query} —Ä–∏—Å–∫–∏ –ø—Ä–æ–±–ª–µ–º—ã –æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
        }
        
        return enhancements.get(analysis_type, query)
    
    async def _get_rag_response(self, query: str, collection_id: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç RAG —Å–∏—Å—Ç–µ–º—ã"""
        try:
            search_request = {
                "query": query,
                "collection_id": collection_id,
                "limit": 5,
                "threshold": 0.5
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.rag_service_url}/documents/search",
                    json=search_request
                ) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RAG –æ—Ç–≤–µ—Ç–∞: {response.status}")
                        return {"error": f"RAG service error: {response.status}"}
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RAG –æ—Ç–≤–µ—Ç–∞: {str(e)}")
            return {"error": str(e)}
    
    async def analyze_with_llm(
        self, 
        chunks: List[TokenChunk], 
        query: str,
        analysis_type: str = 'general'
    ) -> LLMAnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            analysis_type: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ LLM
        """
        start_time = datetime.now()
        logger.info(f"ü§ñ –ù–∞—á–∏–Ω–∞–µ–º LLM –∞–Ω–∞–ª–∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        try:
            # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
            context = self._prepare_llm_context(chunks, query, analysis_type)
            
            # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            prompt = self._create_analysis_prompt(query, analysis_type, context)
            
            # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
            llm_response = await self._query_llm(prompt)
            
            # 4. –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç LLM
            analysis_result = self._parse_llm_response(llm_response, analysis_type)
            
            # 5. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = LLMAnalysisResult(
                analysis_type=analysis_type,
                summary=analysis_result.get('summary', ''),
                key_points=analysis_result.get('key_points', []),
                recommendations=analysis_result.get('recommendations', []),
                confidence=analysis_result.get('confidence', 0.7),
                processing_time=processing_time
            )
            
            logger.info(f"‚úÖ LLM –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            raise
    
    def _prepare_llm_context(self, chunks: List[TokenChunk], query: str, analysis_type: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_chunks = sorted(chunks, key=lambda x: x.importance_score, reverse=True)
        
        # –ë–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —á–∞–Ω–∫–∏
        top_chunks = sorted_chunks[:self.max_chunks_for_analysis]
        
        context_parts = []
        for i, chunk in enumerate(top_chunks):
            context_parts.append(f"–§—Ä–∞–≥–º–µ–Ω—Ç {i+1} (–≤–∞–∂–Ω–æ—Å—Ç—å: {chunk.importance_score:.2f}):")
            context_parts.append(chunk.text)
            context_parts.append("---")
        
        return "\n".join(context_parts)
    
    def _create_analysis_prompt(self, query: str, analysis_type: str, context: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        base_prompt = f"""
–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {query}

–¢–ò–ü –ê–ù–ê–õ–ò–ó–ê: {self.analysis_types.get(analysis_type, '–æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑')}

–ö–û–ù–¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê:
{context}

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –∞–Ω–∞–ª–∏–∑ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:

–ö–†–ê–¢–ö–û–ï –ò–ó–õ–û–ñ–ï–ù–ò–ï:
[–ö—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤, –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ –∑–∞–ø—Ä–æ—Å—É]

–ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:
- [–ö–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç 1]
- [–ö–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç 2]
- [–ö–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç 3]

–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
- [–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1]
- [–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2]

–£–í–ï–†–ï–ù–ù–û–°–¢–¨: [–û—Ü–µ–Ω–∫–∞ –æ—Ç 0.0 –¥–æ 1.0]

–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å—Ç–µ —Ç–æ—á–Ω—ã–º–∏ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏.
"""
        
        return base_prompt
    
    async def _query_llm(self, prompt: str) -> str:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": "llama3.1:8b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.9,
                        "max_tokens": 2048
                    }
                }
                
                async with session.post(
                    f"{self.ollama_service_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.analysis_timeout)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get('response', '')
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ LLM –∑–∞–ø—Ä–æ—Å–∞: {response.status}")
                        return "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"
            
        except asyncio.TimeoutError:
            logger.error("–¢–∞–π–º–∞—É—Ç LLM –∑–∞–ø—Ä–æ—Å–∞")
            return "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ LLM –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
            return f"–û—à–∏–±–∫–∞ LLM: {str(e)}"
    
    def _parse_llm_response(self, response: str, analysis_type: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            result = {
                'summary': '',
                'key_points': [],
                'recommendations': [],
                'confidence': 0.7
            }
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ —Å–µ–∫—Ü–∏—è–º
            sections = response.split('\n\n')
            
            for section in sections:
                section = section.strip()
                if section.startswith('–ö–†–ê–¢–ö–û–ï –ò–ó–õ–û–ñ–ï–ù–ò–ï:'):
                    result['summary'] = section.replace('–ö–†–ê–¢–ö–û–ï –ò–ó–õ–û–ñ–ï–ù–ò–ï:', '').strip()
                elif section.startswith('–ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:'):
                    points_text = section.replace('–ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:', '').strip()
                    result['key_points'] = [point.strip('- ').strip() for point in points_text.split('\n') if point.strip()]
                elif section.startswith('–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:'):
                    rec_text = section.replace('–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:', '').strip()
                    result['recommendations'] = [rec.strip('- ').strip() for rec in rec_text.split('\n') if rec.strip()]
                elif section.startswith('–£–í–ï–†–ï–ù–ù–û–°–¢–¨:'):
                    conf_text = section.replace('–£–í–ï–†–ï–ù–ù–û–°–¢–¨:', '').strip()
                    try:
                        result['confidence'] = float(conf_text)
                    except ValueError:
                        result['confidence'] = 0.7
            
            # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ summary
            if not result['summary'] and not result['key_points']:
                result['summary'] = response
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ LLM –æ—Ç–≤–µ—Ç–∞: {str(e)}")
            return {
                'summary': response,
                'key_points': [],
                'recommendations': [],
                'confidence': 0.5
            }
    
    async def get_comprehensive_analysis(
        self, 
        document_text: str, 
        user_query: str,
        filename: str = None
    ) -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG –∏ LLM
        
        Args:
            document_text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            
        Returns:
            –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {filename}")
        
        try:
            # 1. RAG –∞–Ω–∞–ª–∏–∑
            rag_result = await self.analyze_document_with_rag(document_text, user_query, filename)
            
            # 2. LLM –∞–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            llm_result = await self.analyze_with_llm(rag_result.relevant_chunks, user_query)
            
            # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            comprehensive_result = {
                'query': user_query,
                'filename': filename,
                'rag_analysis': {
                    'confidence': rag_result.confidence_score,
                    'sources_count': len(rag_result.sources),
                    'analysis_time': rag_result.analysis_time,
                    'sources': rag_result.sources
                },
                'llm_analysis': {
                    'analysis_type': llm_result.analysis_type,
                    'summary': llm_result.summary,
                    'key_points': llm_result.key_points,
                    'recommendations': llm_result.recommendations,
                    'confidence': llm_result.confidence,
                    'processing_time': llm_result.processing_time
                },
                'document_stats': {
                    'total_chunks': len(rag_result.relevant_chunks),
                    'total_tokens': sum(chunk.token_count for chunk in rag_result.relevant_chunks),
                    'average_importance': sum(chunk.importance_score for chunk in rag_result.relevant_chunks) / len(rag_result.relevant_chunks) if rag_result.relevant_chunks else 0
                },
                'analysis_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'total_processing_time': rag_result.analysis_time + llm_result.processing_time,
                    'analysis_version': '1.0'
                }
            }
            
            logger.info(f"‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
            return comprehensive_result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            raise

