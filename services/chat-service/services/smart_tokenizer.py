"""
–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import asyncio

logger = logging.getLogger(__name__)

@dataclass
class TokenChunk:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    chunk_id: str
    text: str
    token_count: int
    chunk_type: str  # 'paragraph', 'section', 'table', 'list', 'definition', 'requirement'
    metadata: Dict[str, Any]
    start_position: int
    end_position: int
    parent_section: Optional[str] = None
    importance_score: float = 0.0
    context_keywords: List[str] = None

@dataclass
class DocumentStructure:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞"""
    title: str
    sections: List[Dict[str, Any]]
    total_tokens: int
    chunk_count: int
    document_type: str
    language: str
    metadata: Dict[str, Any]

class SmartTokenizer:
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.max_chunk_size = 1000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
        self.overlap_size = 200     # –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        self.min_chunk_size = 100   # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.section_patterns = [
            r'^(\d+\.?\s+[–ê-–Ø–Å][^.\n]*)',  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
            r'^([–ê-–Ø–Å][^.\n]*\s*:)',       # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –¥–≤–æ–µ—Ç–æ—á–∏–µ–º
            r'^(\d+\.\d+\.?\s+[–ê-–Ø–Å][^.\n]*)',  # –ü–æ–¥—Ä–∞–∑–¥–µ–ª—ã
            r'^(–ì–õ–ê–í–ê\s+\d+[^.\n]*)',      # –ì–ª–∞–≤—ã
            r'^(–†–ê–ó–î–ï–õ\s+\d+[^.\n]*)',     # –†–∞–∑–¥–µ–ª—ã
            r'^(–ü–†–ò–õ–û–ñ–ï–ù–ò–ï\s+[–ê-–Ø0-9][^.\n]*)',  # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.content_patterns = {
            'definition': [
                r'([–ê-–Ø–Å][^.\n]*\s*‚Äî\s*[^.\n]*)',  # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç–∏—Ä–µ
                r'([–ê-–Ø–Å][^.\n]*\s*:\s*[^.\n]*)',   # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥–≤–æ–µ—Ç–æ—á–∏–µ
                r'(–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è|–æ–∑–Ω–∞—á–∞–µ—Ç|–ø–æ–¥\s+[^.\n]*\s+–ø–æ–Ω–∏–º–∞–µ—Ç—Å—è)',
            ],
            'requirement': [
                r'(–¥–æ–ª–∂–µ–Ω|–¥–æ–ª–∂–Ω–∞|–¥–æ–ª–∂–Ω–æ|–¥–æ–ª–∂–Ω—ã)',
                r'(–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ|–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ|—Ç—Ä–µ–±—É–µ—Ç—Å—è)',
                r'(–Ω–µ\s+–¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è|–∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è|–Ω–µ\s+—Ä–∞–∑—Ä–µ—à–∞–µ—Ç—Å—è)',
                r'(—Å–ª–µ–¥—É–µ—Ç|—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)',
            ],
            'table': [
                r'(\|.*\|)',  # –¢–∞–±–ª–∏—Ü—ã –≤ markdown —Ñ–æ—Ä–º–∞—Ç–µ
                r'(\+.*\+)',  # –¢–∞–±–ª–∏—Ü—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
            ],
            'list': [
                r'^(\s*[-‚Ä¢*]\s+[^.\n]*)',  # –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
                r'^(\s*\d+\.\s+[^.\n]*)',  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            ],
            'formula': [
                r'(\$.*?\$)',  # LaTeX —Ñ–æ—Ä–º—É–ª—ã
                r'(\[.*?\])',  # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            ],
            'reference': [
                r'(–ì–û–°–¢\s+\d+(?:\.\d+)*-\d{4})',
                r'(–°–ü\s+\d+(?:\.\d+)*\.\d{4})',
                r'(–°–ù–∏–ü\s+\d+(?:\.\d+)*-\d{4})',
                r'(–ø\.\s*\d+(?:\.\d+)*)',
                r'(—Ä–∞–∑–¥–µ–ª\s+\d+(?:\.\d+)*)',
            ]
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
        self.importance_keywords = {
            'high': ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è', '–¥–æ–ª–∂–µ–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ'],
            'medium': ['—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '—Å–ª–µ–¥—É–µ—Ç', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ', '–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ'],
            'low': ['–º–æ–∂–µ—Ç', '–≤–æ–∑–º–æ–∂–Ω–æ', '–¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è', '—Ä–∞–∑—Ä–µ—à–∞–µ—Ç—Å—è']
        }
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.stop_words = {
            'ru': ['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'],
            'en': ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their']
        }
    
    async def tokenize_document(self, text: str, filename: str = None) -> Tuple[List[TokenChunk], DocumentStructure]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            filename: –ò–º—è —Ñ–∞–π–ª–∞ (–¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º —É–º–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞: {filename}")
        
        try:
            # 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = self._preprocess_text(text)
            
            # 2. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_structure = await self._analyze_document_structure(cleaned_text, filename)
            
            # 3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
            language = self._detect_language(cleaned_text)
            
            # 4. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
            semantic_blocks = await self._extract_semantic_blocks(cleaned_text, document_structure)
            
            # 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–ª–æ–∫–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            token_chunks = await self._tokenize_blocks(semantic_blocks, language)
            
            # 6. –û–±–æ–≥–∞—â–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            enriched_chunks = await self._enrich_chunks_metadata(token_chunks, document_structure)
            
            # 7. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤
            optimized_chunks = await self._optimize_chunk_sizes(enriched_chunks)
            
            logger.info(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(optimized_chunks)} —á–∞–Ω–∫–æ–≤, {document_structure.total_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
            
            return optimized_chunks, document_structure
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
            raise
    
    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    async def _analyze_document_structure(self, text: str, filename: str = None) -> DocumentStructure:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        sections = []
        current_section = None
        
        lines = text.split('\n')
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Ä–∞–∑–¥–µ–ª–∞
            for pattern in self.section_patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    if current_section:
                        sections.append(current_section)
                    
                    current_section = {
                        'title': match.group(1),
                        'start_line': line_num,
                        'end_line': line_num,
                        'level': self._get_section_level(line),
                        'content': []
                    }
                    break
            else:
                if current_section:
                    current_section['content'].append(line)
                    current_section['end_line'] = line_num
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
        if current_section:
            sections.append(current_section)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_type = self._detect_document_type(text, filename)
        
        return DocumentStructure(
            title=self._extract_title(text, filename),
            sections=sections,
            total_tokens=len(text.split()),
            chunk_count=0,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ
            document_type=document_type,
            language=self._detect_language(text),
            metadata={
                'filename': filename,
                'sections_count': len(sections),
                'analyzed_at': datetime.now().isoformat()
            }
        )
    
    def _get_section_level(self, line: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        if re.match(r'^\d+\.\d+\.\d+', line):
            return 3
        elif re.match(r'^\d+\.\d+', line):
            return 2
        elif re.match(r'^\d+\.', line):
            return 1
        else:
            return 0
    
    def _detect_document_type(self, text: str, filename: str = None) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        text_lower = text.lower()
        
        if any(keyword in text_lower for keyword in ['–≥–æ—Å—Ç', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è']):
            return 'standard'
        elif any(keyword in text_lower for keyword in ['—Å–Ω–∏–ø', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã']):
            return 'building_code'
        elif any(keyword in text_lower for keyword in ['—Å–ø', '—Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª']):
            return 'code_of_practice'
        elif any(keyword in text_lower for keyword in ['—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ', '—Ç–∑']):
            return 'technical_specification'
        elif any(keyword in text_lower for keyword in ['–ø—Ä–æ–µ–∫—Ç', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ']):
            return 'project_document'
        else:
            return 'general'
    
    def _extract_title(self, text: str, filename: str = None) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        lines = text.split('\n')
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
        for line in lines[:10]:
            line = line.strip()
            if len(line) > 10 and len(line) < 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if not any(keyword in line.lower() for keyword in ['—Å—Ç—Ä–∞–Ω–∏—Ü–∞', 'page', '–¥–∞—Ç–∞', 'date']):
                    return line
        
        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        if filename:
            return filename.replace('.pdf', '').replace('.docx', '').replace('.txt', '')
        
        return "–î–æ–∫—É–º–µ–Ω—Ç"
    
    def _detect_language(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        cyrillic_count = len(re.findall(r'[–∞-—è—ë]', text.lower()))
        latin_count = len(re.findall(r'[a-z]', text.lower()))
        
        if cyrillic_count > latin_count:
            return 'ru'
        else:
            return 'en'
    
    async def _extract_semantic_blocks(self, text: str, structure: DocumentStructure) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        blocks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs):
            if len(paragraph) < 20:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_type = self._classify_content_type(paragraph)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
            importance = self._calculate_importance(paragraph)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_keywords(paragraph)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —Ä–∞–∑–¥–µ–ª
            parent_section = self._find_parent_section(paragraph, structure)
            
            blocks.append({
                'text': paragraph,
                'type': content_type,
                'importance': importance,
                'keywords': keywords,
                'parent_section': parent_section,
                'position': i,
                'length': len(paragraph)
            })
        
        return blocks
    
    def _classify_content_type(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        text_lower = text.lower()
        
        for content_type, patterns in self.content_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    return content_type
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞
        if len(text.split()) < 20:
            return 'short_text'
        elif any(char in text for char in ['‚Ä¢', '-', '*', '1.', '2.']):
            return 'list'
        elif '|' in text or '+' in text:
            return 'table'
        else:
            return 'paragraph'
    
    def _calculate_importance(self, text: str) -> float:
        """–†–∞—Å—á–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        importance_score = 0.5  # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
        for level, keywords in self.importance_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    if level == 'high':
                        importance_score += 0.3
                    elif level == 'medium':
                        importance_score += 0.1
                    else:
                        importance_score -= 0.1
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        if len(text) > 500:
            importance_score += 0.1
        elif len(text) < 100:
            importance_score -= 0.1
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ñ—Ä –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        if re.search(r'\d+', text):
            importance_score += 0.1
        
        return max(0.0, min(1.0, importance_score))
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words = re.findall(r'\b[–∞-—è—ë]{3,}\b', text.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = self.stop_words.get('ru', [])
        keywords = [word for word in words if word not in stop_words]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        word_freq = {}
        for word in keywords:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        return sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)[:10]
    
    def _find_parent_section(self, text: str, structure: DocumentStructure) -> Optional[str]:
        """–ü–æ–∏—Å–∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ - –∏—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π —Ä–∞–∑–¥–µ–ª –≤—ã—à–µ
        for section in reversed(structure.sections):
            if any(keyword in text.lower() for keyword in section['title'].lower().split()):
                return section['title']
        
        return None
    
    async def _tokenize_blocks(self, blocks: List[Dict[str, Any]], language: str) -> List[TokenChunk]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–ª–æ–∫–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        chunks = []
        
        for i, block in enumerate(blocks):
            text = block['text']
            
            # –ï—Å–ª–∏ –±–ª–æ–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
            if len(text.split()) > self.max_chunk_size:
                sub_chunks = await self._split_large_block(text, block)
                chunks.extend(sub_chunks)
            else:
                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫ –∏–∑ –±–ª–æ–∫–∞
                chunk = TokenChunk(
                    chunk_id=f"chunk_{i}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    text=text,
                    token_count=len(text.split()),
                    chunk_type=block['type'],
                    metadata={
                        'importance': block['importance'],
                        'keywords': block['keywords'],
                        'parent_section': block['parent_section'],
                        'position': block['position'],
                        'language': language
                    },
                    start_position=0,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                    end_position=len(text),
                    parent_section=block['parent_section'],
                    importance_score=block['importance'],
                    context_keywords=block['keywords']
                )
                chunks.append(chunk)
        
        return chunks
    
    async def _split_large_block(self, text: str, block: Dict[str, Any]) -> List[TokenChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –±–ª–æ–∫–∞ –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞–Ω–∫–∏"""
        chunks = []
        sentences = re.split(r'[.!?]+', text)
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_tokens = len(sentence.split())
            
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
            if current_tokens + sentence_tokens > self.max_chunk_size and current_chunk:
                chunk = TokenChunk(
                    chunk_id=f"chunk_{block['position']}_{chunk_index}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    text=current_chunk.strip(),
                    token_count=current_tokens,
                    chunk_type=block['type'],
                    metadata={
                        'importance': block['importance'],
                        'keywords': block['keywords'],
                        'parent_section': block['parent_section'],
                        'position': block['position'],
                        'chunk_index': chunk_index,
                        'is_split': True
                    },
                    start_position=0,
                    end_position=len(current_chunk),
                    parent_section=block['parent_section'],
                    importance_score=block['importance'],
                    context_keywords=block['keywords']
                )
                chunks.append(chunk)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + " " + sentence
                current_tokens = len(current_chunk.split())
                chunk_index += 1
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_tokens += sentence_tokens
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunk = TokenChunk(
                chunk_id=f"chunk_{block['position']}_{chunk_index}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                text=current_chunk.strip(),
                token_count=current_tokens,
                chunk_type=block['type'],
                metadata={
                    'importance': block['importance'],
                    'keywords': block['keywords'],
                    'parent_section': block['parent_section'],
                    'position': block['position'],
                    'chunk_index': chunk_index,
                    'is_split': True
                },
                start_position=0,
                end_position=len(current_chunk),
                parent_section=block['parent_section'],
                importance_score=block['importance'],
                context_keywords=block['keywords']
            )
            chunks.append(chunk)
        
        return chunks
    
    def _get_overlap_text(self, text: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏"""
        words = text.split()
        if len(words) <= self.overlap_size:
            return text
        
        return " ".join(words[-self.overlap_size:])
    
    async def _enrich_chunks_metadata(self, chunks: List[TokenChunk], structure: DocumentStructure) -> List[TokenChunk]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        for i, chunk in enumerate(chunks):
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            chunk.metadata['document_position'] = i
            chunk.metadata['document_type'] = structure.document_type
            chunk.metadata['document_title'] = structure.title
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–∞—Ö
            if i > 0:
                chunk.metadata['previous_chunk_id'] = chunks[i-1].chunk_id
            if i < len(chunks) - 1:
                chunk.metadata['next_chunk_id'] = chunks[i+1].chunk_id
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            chunk.metadata['created_at'] = datetime.now().isoformat()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–∏
            chunk.start_position = sum(len(c.text) for c in chunks[:i])
            chunk.end_position = chunk.start_position + len(chunk.text)
        
        return chunks
    
    async def _optimize_chunk_sizes(self, chunks: List[TokenChunk]) -> List[TokenChunk]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤"""
        optimized_chunks = []
        
        for chunk in chunks:
            # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, –ø—ã—Ç–∞–µ–º—Å—è –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏
            if chunk.token_count < self.min_chunk_size and len(optimized_chunks) > 0:
                last_chunk = optimized_chunks[-1]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å
                if (last_chunk.token_count + chunk.token_count <= self.max_chunk_size and
                    last_chunk.chunk_type == chunk.chunk_type and
                    last_chunk.parent_section == chunk.parent_section):
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏
                    last_chunk.text += " " + chunk.text
                    last_chunk.token_count += chunk.token_count
                    last_chunk.end_position = chunk.end_position
                    last_chunk.metadata['merged_chunks'] = last_chunk.metadata.get('merged_chunks', 1) + 1
                    last_chunk.metadata['merged_chunk_ids'] = last_chunk.metadata.get('merged_chunk_ids', [last_chunk.chunk_id]) + [chunk.chunk_id]
                    continue
            
            optimized_chunks.append(chunk)
        
        return optimized_chunks
    
    def get_tokenization_stats(self, chunks: List[TokenChunk], structure: DocumentStructure) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
        total_tokens = sum(chunk.token_count for chunk in chunks)
        avg_chunk_size = total_tokens / len(chunks) if chunks else 0
        
        chunk_types = {}
        for chunk in chunks:
            chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1
        
        return {
            'total_chunks': len(chunks),
            'total_tokens': total_tokens,
            'average_chunk_size': avg_chunk_size,
            'chunk_types_distribution': chunk_types,
            'document_type': structure.document_type,
            'language': structure.language,
            'sections_count': len(structure.sections),
            'tokenization_time': datetime.now().isoformat()
        }

