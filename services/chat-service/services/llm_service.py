"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ ollama-service
"""

import httpx
import logging
import asyncio
from typing import Dict, Any, Optional, AsyncGenerator
from datetime import datetime

logger = logging.getLogger(__name__)

class LLMService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM —á–µ—Ä–µ–∑ ollama-service"""
    
    def __init__(self, ollama_service_url: str = "http://ollama-service:8012"):
        self.ollama_service_url = ollama_service_url
        self.timeout = httpx.Timeout(300.0)  # 5 –º–∏–Ω—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
    async def generate_response(
        self,
        prompt: str,
        model: str = "llama3.1:8b",
        temperature: float = 0.7,
        max_tokens: int = 2048,
        top_p: float = 0.9,
        top_k: int = 40,
        repeat_penalty: float = 1.1,
        system_prompt: str = "",
        timeout: Optional[float] = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            model: –ú–æ–¥–µ–ª—å LLM
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            top_p: Top-p –ø–∞—Ä–∞–º–µ—Ç—Ä
            top_k: Top-k –ø–∞—Ä–∞–º–µ—Ç—Ä
            repeat_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            timeout: –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç LLM
        """
        logger.info(f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM. –ú–æ–¥–µ–ª—å: {model}, –¢–æ–∫–µ–Ω—ã: {max_tokens}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
        request_timeout = timeout or 300.0
        timeout_config = httpx.Timeout(request_timeout)
        
        try:
            async with httpx.AsyncClient(timeout=timeout_config) as client:
                payload = {
                    "prompt": prompt,
                    "max_tokens": max_tokens
                }
                
                logger.debug(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ ollama-service: {payload}")
                
                response = await client.post(
                    f"{self.ollama_service_url}/models/{model}/generate",
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    # Ollama-service –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –≤ data.response
                    llm_response = result.get("data", {}).get("response", result.get("response", ""))
                    logger.info(f"‚úÖ –û—Ç–≤–µ—Ç LLM –ø–æ–ª—É—á–µ–Ω. –î–ª–∏–Ω–∞: {len(llm_response)}")
                    return llm_response if llm_response else "–û—à–∏–±–∫–∞: –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM"
                else:
                    error_msg = f"–û—à–∏–±–∫–∞ ollama-service: {response.status_code} - {response.text}"
                    logger.error(f"‚ùå {error_msg}")
                    return f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM: {error_msg}"
                    
        except httpx.TimeoutException:
            error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM ({request_timeout}s)"
            logger.error(f"‚è∞ {error_msg}")
            return f"–¢–∞–π–º–∞—É—Ç: {error_msg}"
        except httpx.ConnectError:
            error_msg = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ ollama-service"
            logger.error(f"üîå {error_msg}")
            return f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {error_msg}"
        except Exception as e:
            error_msg = f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
            logger.error(f"üí• {error_msg}")
            return f"–û—à–∏–±–∫–∞: {error_msg}"
    
    async def generate_streaming_response(
        self,
        prompt: str,
        model: str = "llama3.1:8b",
        temperature: float = 0.7,
        max_tokens: int = 2048,
        top_p: float = 0.9,
        top_k: int = 40,
        repeat_penalty: float = 1.1,
        system_prompt: str = "",
        timeout: Optional[float] = None
    ) -> AsyncGenerator[str, None]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            model: –ú–æ–¥–µ–ª—å LLM
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            top_p: Top-p –ø–∞—Ä–∞–º–µ—Ç—Ä
            top_k: Top-k –ø–∞—Ä–∞–º–µ—Ç—Ä
            repeat_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            timeout: –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Yields:
            –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
        """
        logger.info(f"üåä –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM. –ú–æ–¥–µ–ª—å: {model}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
        request_timeout = timeout or 300.0
        timeout_config = httpx.Timeout(request_timeout)
        
        try:
            async with httpx.AsyncClient(timeout=timeout_config) as client:
                payload = {
                    "prompt": prompt,
                    "max_tokens": max_tokens
                }
                
                async with client.stream(
                    "POST",
                    f"{self.ollama_service_url}/models/{model}/generate",
                    json=payload
                ) as response:
                    if response.status_code == 200:
                        async for line in response.aiter_lines():
                            if line.strip():
                                try:
                                    data = response.json()
                                    if "response" in data:
                                        yield data["response"]
                                except:
                                    # –ï—Å–ª–∏ –Ω–µ JSON, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                                    yield line
                    else:
                        error_msg = f"–û—à–∏–±–∫–∞ ollama-service: {response.status_code}"
                        logger.error(f"‚ùå {error_msg}")
                        yield f"–û—à–∏–±–∫–∞: {error_msg}"
                        
        except httpx.TimeoutException:
            error_msg = f"–¢–∞–π–º–∞—É—Ç –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM ({request_timeout}s)"
            logger.error(f"‚è∞ {error_msg}")
            yield f"–¢–∞–π–º–∞—É—Ç: {error_msg}"
        except httpx.ConnectError:
            error_msg = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ ollama-service"
            logger.error(f"üîå {error_msg}")
            yield f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {error_msg}"
        except Exception as e:
            error_msg = f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
            logger.error(f"üí• {error_msg}")
            yield f"–û—à–∏–±–∫–∞: {error_msg}"
    
    async def get_available_models(self) -> list:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        logger.info("üìã –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(30.0)) as client:
                response = await client.get(f"{self.ollama_service_url}/models")
                
                if response.status_code == 200:
                    result = response.json()
                    # Ollama-service –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ –º–æ–¥–µ–ª–µ–π –Ω–∞–ø—Ä—è–º—É—é
                    if isinstance(result, list):
                        models = [model.get("name", model) if isinstance(model, dict) else model for model in result]
                    else:
                        models = result.get("models", [])
                    
                    logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π: {models}")
                    return models
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {response.status_code}")
                    return []
                    
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {str(e)}")
            return []
    
    async def check_model_availability(self, model: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            
        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞
        """
        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏: {model}")
        
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(30.0)) as client:
                response = await client.get(f"{self.ollama_service_url}/models/{model}")
                
                if response.status_code == 200:
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model} –¥–æ—Å—Ç—É–ø–Ω–∞")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {response.status_code}")
                    return False
                    
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏ {model}: {str(e)}")
            return False
