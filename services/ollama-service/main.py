"""
Ollama Management Service
–°–µ—Ä–≤–∏—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ Ollama
"""

from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
import httpx
import os
import sys
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
import logging
import traceback
from datetime import datetime
from contextlib import asynccontextmanager

# –ò–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
from logging_utils import setup_service_logging, log_request, log_error, log_performance, log_business_event

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = setup_service_logging("ollama-service")

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("üöÄ Ollama Management Service –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    logger.info(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama: {OLLAMA_BASE_URL}")
    yield
    # Shutdown
    logger.info("üõë Ollama Management Service –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è...")

app = FastAPI(
    title="Ollama Management Service",
    description="–°–µ—Ä–≤–∏—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ Ollama",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Middleware –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
@app.middleware("http")
async def logging_middleware(request: Request, call_next):
    start_time = datetime.now()
    request_id = f"ollama_req_{start_time.strftime('%Y%m%d_%H%M%S_%f')}"
    
    logger.info(f"üì• –í—Ö–æ–¥—è—â–∏–π –∑–∞–ø—Ä–æ—Å: {request.method} {request.url.path}", extra={
        "request_id": request_id,
        "method": request.method,
        "path": request.url.path,
        "query_params": str(request.query_params)
    })
    
    try:
        response = await call_next(request)
        process_time = (datetime.now() - start_time).total_seconds()
        
        log_request(
            logger=logger,
            method=request.method,
            path=request.url.path,
            status_code=response.status_code,
            duration=process_time,
            request_id=request_id
        )
        
        return response
        
    except Exception as e:
        process_time = (datetime.now() - start_time).total_seconds()
        log_error(
            logger=logger,
            error=e,
            context=f"HTTP {request.method} {request.url.path}",
            request_id=request_id
        )
        raise

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OLLAMA_BASE_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

class ModelInfo(BaseModel):
    name: str
    size: int
    digest: str
    modified_at: str

class ModelRequest(BaseModel):
    name: str
    stream: bool = False

class ModelResponse(BaseModel):
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None

class SettingsRequest(BaseModel):
    default_model: str
    available_models: List[str]

class SettingsResponse(BaseModel):
    default_model: str
    available_models: List[str]
    current_model: str

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    logger.info("üè• Health check –∑–∞–ø—Ä–æ—Å")
    return {"status": "healthy", "service": "ollama-management-service"}

@app.get("/models", response_model=List[ModelInfo])
async def get_models():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    start_time = datetime.now()
    logger.info("üìã –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π Ollama")
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            response.raise_for_status()
            data = response.json()
            
            models = []
            for model in data.get("models", []):
                models.append(ModelInfo(
                    name=model["name"],
                    size=model["size"],
                    digest=model["digest"],
                    modified_at=model["modified_at"]
                ))
            
            duration = (datetime.now() - start_time).total_seconds()
            log_performance(
                logger=logger,
                operation="get_models",
                duration=duration,
                success=True,
                models_count=len(models)
            )
            
            log_business_event(
                logger=logger,
                event="models_listed",
                models_count=len(models)
            )
            
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π")
            return models
            
    except httpx.RequestError as e:
        duration = (datetime.now() - start_time).total_seconds()
        log_performance(
            logger=logger,
            operation="get_models",
            duration=duration,
            success=False
        )
        log_error(
            logger=logger,
            error=e,
            context="get_models - –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"
        )
        raise HTTPException(status_code=503, detail="Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        log_performance(
            logger=logger,
            operation="get_models",
            duration=duration,
            success=False
        )
        log_error(
            logger=logger,
            error=e,
            context="get_models"
        )
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")

@app.post("/models/pull", response_model=ModelResponse)
async def pull_model(model_request: ModelRequest):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        async with httpx.AsyncClient(timeout=300.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/pull",
                json={"name": model_request.name, "stream": model_request.stream}
            )
            response.raise_for_status()
            
            return ModelResponse(
                success=True,
                message=f"–ú–æ–¥–µ–ª—å {model_request.name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
                data={"model": model_request.name}
            )
    except httpx.RequestError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
        raise HTTPException(status_code=503, detail="Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏")

@app.delete("/models/{model_name}", response_model=ModelResponse)
async def delete_model(model_name: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.delete(f"{OLLAMA_BASE_URL}/api/delete", 
                                         json={"name": model_name})
            response.raise_for_status()
            
            return ModelResponse(
                success=True,
                message=f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞"
            )
    except httpx.RequestError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
        raise HTTPException(status_code=503, detail="Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100

@app.post("/models/{model_name}/generate", response_model=ModelResponse)
async def generate_text(model_name: str, request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏"""
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": model_name,
                    "prompt": request.prompt,
                    "stream": False,
                    "options": {
                        "num_predict": request.max_tokens
                    }
                }
            )
            response.raise_for_status()
            data = response.json()
            
            return ModelResponse(
                success=True,
                message="–¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω",
                data={
                    "response": data.get("response", ""),
                    "model": model_name,
                    "prompt": request.prompt
                }
            )
    except httpx.RequestError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
        raise HTTPException(status_code=503, detail="Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞")

@app.get("/settings", response_model=SettingsResponse)
async def get_settings():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–µ–π"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        models_response = await get_models()
        available_models = [model.name for model in models_response]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å (–º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ –ë–î –∏–ª–∏ —Ñ–∞–π–ª–µ)
        default_model = os.getenv("DEFAULT_MODEL", "llama3.1:8b")
        
        return SettingsResponse(
            default_model=default_model,
            available_models=available_models,
            current_model=default_model
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫")

@app.post("/settings", response_model=ModelResponse)
async def update_settings(settings: SettingsRequest):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–µ–π"""
    try:
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –ë–î –∏–ª–∏ —Ñ–∞–π–ª
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å–ø–µ—Ö
        
        return ModelResponse(
            success=True,
            message="–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã",
            data={
                "default_model": settings.default_model,
                "available_models": settings.available_models
            }
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫")

@app.get("/status")
async def get_ollama_status():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ Ollama"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            response.raise_for_status()
            
            return {
                "status": "connected",
                "ollama_url": OLLAMA_BASE_URL,
                "models_count": len(response.json().get("models", []))
            }
    except httpx.RequestError as e:
        logger.error(f"Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return {
            "status": "disconnected",
            "ollama_url": OLLAMA_BASE_URL,
            "error": str(e)
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8012)
