#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ingest —Å–∏—Å—Ç–µ–º—ã
"""

import asyncio
import logging
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.append(str(Path(__file__).parent))

from pipeline.parser import DocumentParser
from pipeline.vectorizer import DocumentVectorizer
from rag.service.hybrid_search import HybridSearchService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã ingest —Å–∏—Å—Ç–µ–º—ã"""
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ ingest —Å–∏—Å—Ç–µ–º—ã...")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        parser = DocumentParser()
        vectorizer = DocumentVectorizer()
        search_service = HybridSearchService()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ Qdrant
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ Qdrant...")
        await vectorizer.ensure_collections_exist()
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        test_documents = [
            {
                "file_path": "test_pump_spec.pdf",
                "content": "–¶–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –Ω–∞—Å–æ—Å API 610 –¥–ª—è –ø–µ—Ä–µ–∫–∞—á–∫–∏ –∞–º–º–∏–∞–∫–∞. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 1000 –º3/—á, –Ω–∞–ø–æ—Ä 50 –º, –º–æ—â–Ω–æ—Å—Ç—å 75 –∫–í—Ç. –ú–∞—Ç–µ—Ä–∏–∞–ª –∫–æ—Ä–ø—É—Å–∞ - –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞–ª—å 316L. –ü–æ—Å—Ç–∞–≤—â–∏–∫: Sulzer.",
                "metadata": {
                    "project_id": "EC-Karat-2021",
                    "object_id": "NH3-ATR-3500tpd",
                    "discipline": "process",
                    "doc_no": "AI-ENG-SPEC-001",
                    "rev": "A",
                    "chunk_type": "text"
                }
            },
            {
                "file_path": "test_heat_exchanger_spec.pdf", 
                "content": "–¢–µ–ø–ª–æ–æ–±–º–µ–Ω–Ω–∏–∫ –∫–æ–∂—É—Ö–æ—Ç—Ä—É–±–Ω—ã–π TEMA E. –¢–µ–ø–ª–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ 5000 –∫–í—Ç, –ø–ª–æ—â–∞–¥—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ 200 –º2. –ú–∞—Ç–µ—Ä–∏–∞–ª –∫–æ–∂—É—Ö–∞ - —É–≥–ª–µ—Ä–æ–¥–∏—Å—Ç–∞—è —Å—Ç–∞–ª—å A106, —Ç—Ä—É–± - –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞–ª—å 316L. –ü–æ—Å—Ç–∞–≤—â–∏–∫: Alfa Laval.",
                "metadata": {
                    "project_id": "EC-Karat-2021",
                    "object_id": "NH3-ATR-3500tpd",
                    "discipline": "process",
                    "doc_no": "AI-ENG-SPEC-002",
                    "rev": "A",
                    "chunk_type": "text"
                }
            }
        ]
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        parsed_documents = []
        
        for doc in test_documents:
            result = await parser.parse_document(doc["file_path"], doc["metadata"])
            result["content"] = doc["content"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            parsed_documents.append(result)
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {doc['file_path']}")
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        logger.info("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        for doc in parsed_documents:
            chunks = [{
                "chunk_id": f"chunk_{doc['file_path']}",
                "content": doc["content"],
                **doc["metadata"]
            }]
            
            await vectorizer.vectorize_chunks(chunks, "ae_text_m3")
            logger.info(f"–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {doc['file_path']}")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞
        logger.info("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
        
        # –ü–æ–∏—Å–∫ –Ω–∞—Å–æ—Å–æ–≤
        pump_results = await search_service.hybrid_search(
            query="—Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –Ω–∞—Å–æ—Å –∞–º–º–∏–∞–∫",
            limit=5
        )
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –Ω–∞—Å–æ—Å–æ–≤: {len(pump_results)}")
        for i, result in enumerate(pump_results):
            logger.info(f"  {i+1}. Score: {result['combined_score']:.3f}")
            logger.info(f"     Content: {result['content'][:100]}...")
        
        # –ü–æ–∏—Å–∫ —Ç–µ–ø–ª–æ–æ–±–º–µ–Ω–Ω–∏–∫–æ–≤
        heat_exchanger_results = await search_service.hybrid_search(
            query="—Ç–µ–ø–ª–æ–æ–±–º–µ–Ω–Ω–∏–∫ –∫–æ–∂—É—Ö–æ—Ç—Ä—É–±–Ω—ã–π TEMA",
            limit=5
        )
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–µ–ø–ª–æ–æ–±–º–µ–Ω–Ω–∏–∫–æ–≤: {len(heat_exchanger_results)}")
        for i, result in enumerate(heat_exchanger_results):
            logger.info(f"  {i+1}. Score: {result['combined_score']:.3f}")
            logger.info(f"     Content: {result['content'][:100]}...")
        
        # –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        logger.info("–ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è...")
        
        analog_results = await search_service.search_analogs(
            equipment_description="—Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –Ω–∞—Å–æ—Å",
            equipment_type="pump",
            parameters={"flow_rate": 1000, "head": 50},
            limit=3
        )
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤: {len(analog_results)}")
        for i, result in enumerate(analog_results):
            logger.info(f"  {i+1}. Relevance: {result['relevance_score']:.3f}")
            logger.info(f"     Equipment: {result['content'][:100]}...")
        
        logger.info("‚úÖ Ingest —Å–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ ingest —Å–∏—Å—Ç–µ–º–µ: {str(e)}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
