# 🚀 Быстрый запуск AI Engineering Platform

## Предварительные требования

1. **Docker и Docker Compose** установлены
2. **Ollama** установлен на хосте

## Пошаговый запуск

### 1. Установка Ollama (если не установлен)

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Скачать с https://ollama.ai/download
```

### 2. Запуск Ollama на хосте

```bash
# Запуск Ollama сервера
ollama serve

# В другом терминале - загрузка модели
ollama pull llama2
```

### 3. Запуск всех сервисов

```bash
# Клонирование репозитория (если еще не сделано)
git clone <repository-url>
cd AI-Engineering

# Запуск всех контейнеров
docker-compose up -d
```

### 4. Проверка работы

- **Веб-интерфейс:** http://localhost:3000
- **Дашборд:** http://localhost:3000/ (главная страница)
- **API документация:** http://localhost:8003/docs

## Архитектура

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Frontend      │    │   Nginx         │    │   PostgreSQL    │
│   (React)       │    │   (Proxy)       │    │   (Database)    │
│   :3000         │    │   :80           │    │   :5432         │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
         ┌───────────────────────┼───────────────────────┐
         │                       │                       │
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Qdrant        │    │   Redis         │    │   MinIO         │
│   (Vector DB)   │    │   (Cache)       │    │   (Files)       │
│   :6333-6334    │    │   :6379         │    │   :9000-9001    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
         ┌───────────────────────┼───────────────────────┐
         │                       │                       │
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   vLLM          │    │   RabbitMQ      │    │   Microservices │
│   (LLM Server)  │    │   (Queue)       │    │   (8001-8011)   │
│   :8002         │    │   :5672,15672   │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │
         │
┌─────────────────┐
│   Ollama        │
│   (Host)        │
│   :11434        │
└─────────────────┘
```

## Модули системы

- **📊 Дашборд** - Мониторинг сервисов
- **💬 Чат с ИИ** - Общение с ИИ
- **📚 Консультации НТД** - Нормативно-техническая документация
- **📁 Объекты аналоги** - Архив и база данных
- **🧮 Инженерные расчеты** - Вычисления и анализ
- **✅ Проверка данных** - Валидация входных данных
- **📄 Документы** - Управление документами
- **📈 Аналитика** - Отчетность и аналитика
- **🔗 Интеграции** - PLM и внешние системы
- **📤 Выходной контроль** - Проверка исходящих документов
- **⚙️ Настройки** - Конфигурация системы

## Полезные команды

```bash
# Просмотр логов
docker-compose logs -f [service-name]

# Перезапуск сервиса
docker-compose restart [service-name]

# Остановка всех сервисов
docker-compose down

# Остановка с удалением volumes
docker-compose down -v
```

## Устранение неполадок

### Ollama не отвечает
```bash
# Проверить статус Ollama
ollama list

# Перезапустить Ollama
ollama serve
```

### Проблемы с портами
```bash
# Проверить занятые порты
lsof -i :3000
lsof -i :8002
```

### Проблемы с Docker
```bash
# Очистка Docker
docker system prune -a
docker-compose down -v
docker-compose up -d
```
